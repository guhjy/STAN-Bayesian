\documentclass[12pt, a4paper]{report}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\bibliographystyle{chicago}
\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\begin{document}
\author{Kevin O'Brien}
\title{Spring 2011}

\addcontentsline{toc}{section}{Bibliography}

%----------------------------------------------------------------------------------------%
%----------------------------------------------------------------------------------------%
\newpage
\section{Statement of the LME model}

Further to a paper published by Laird and Ware in $1982$, a linear mixed effects model is a linear mdoel that combined fixed and random effect terms formulated as follows;

  \begin{displaymath}
      Y_{i} =X_{i}\beta + Z_{i}b_{i} + \epsilon_{i}
  \end{displaymath}
\begin{itemize}

\item $Y_{i}$ is the $n \times 1$ response vector \item $X_{i}$ is
the $n \times p$ Model matrix for fixed effects \item $\beta$ is
the $p \times 1$ vector of fixed effects coefficients \item
$Z_{i}$ is the $n \times q$ Model matrix for random effects \item
$b_{i}$ is the $q \times 1$ vector of random effects coefficients,
sometimes denoted as $u_{i}$ \item $\epsilon$ is the $n \times 1$
vector of observation errors
\end{itemize}


\section{The Linear Mixed Effects Model}
The linear mixed effects model is given by
\begin{equation}
Y = X\beta + Zu + \epsilon
\end{equation}


\textbf{Y} is the vector of $n$ observations, with dimension $n
\times 1$. \textbf{b} is a vector of fixed $p$ effects, and has
dimension $p \times 1$. It is composed of coefficients, with the
first element being the population mean.  \textbf{X} is known as
the design `matrix', model matrix for fixed effects, and comprises
$0$s or $1$s, depending on whether the relevant fixed effects have
any effect on the observation is question. \textbf{X} has
dimension $n \times p$. \textbf{e} is the vector of residuals with
dimension $n \times 1$.

The random effects models can be specified similarly. \textbf{Z}
is known as the `model matrix for random effects', and also
comprises $0$s or $1$s. It has dimension $n \times q$. \textbf{u
}is a vector of random $q$ effects, and has dimension $q \times
1$.


% \subsection{Formulation of the Variance Matrix V}
\textbf{V} , the variance matrix of \textbf{Y}, can be expressed
as follows;
\begin{eqnarray}
\textbf{V}= var ( \textbf{Xb} + \textbf{Zu} + \textbf{e})\\
\textbf{V}= var ( \textbf{Xb} ) + var (\textbf{Zu}) +
var(\textbf{e}))
\end{eqnarray}

$\mbox{var}(\textbf{Xb})$ is known to be zero. The variance of the
random effects $\mbox{var}(\textbf{Zu})$ can be written as
$Z\mbox{var}(\textbf{u})Z^{T}$.

By letting var$(u) = G$ (i.e $\textbf{u} ~ N(0,\textbf{G})$), this
becomes $ZGZ^{T}$. This specifies the covariance due to random
effects. The residual covariance matrix $var(e)$ is denoted as
$R$, ($\textbf{e} ~ N(0,\textbf{R})$). Residual are uncorrelated,
hence \textbf{R} is equivalent to $\sigma^{2}$\textbf{I}, where
\textbf{I} is the identity matrix. The variance matrix \textbf{V}
can therefore be written as;

\begin{equation}
\textbf{V}  = ZGZ^{T} + \textbf{R}
\end{equation}

%\subsection{Estimators and Predictors}

The best linear unbiased predictor (BLUP) is used to estimating
random effects, i.e to derive \textbf{u}. The best linear unbiased
estimator (BLUE) is used to estimate the fixed effects,
\textbf{b}. They were formulated in a paper by \cite{Henderson59},
which provides the derivations of both. Inferences about fixed
effects have come to be called `estimates', whereas inferences
about random effects have come be called `predictions`. hence the
naming of BLUP is to reinforce distinction between the two , but
it is essentially the same principal involved in both cases
\citep{Robinson}. The BLUE of \textbf{b}, and the BLUP of
\textbf{u} can be shown to be;

\begin{equation}
\hat{b} = (X^{T}V^{-1}X)^{-1}X^{T}V^{-1}y
\end{equation}
\begin{equation}
\hat{u} = GZ^{T}V^{-1}(y-X\hat{b})
\end{equation}

The practical application of both expressions requires that the
variance components be known. An estimate for the variance
components must be derived to  either maximum likelihood (ML) or
more commonly restricted maximum likelihood (REML).

Importantly calculations based on the above formulae require the
calculation of the inverse of \textbf{V}. In simple examples
$V^{-1}$ is a straightforward calculation, but with higher
dimensions it becomes a very complex calculation.




\subsection
The classical model is based on measurements $y_{mi}$
by method $m=1,2$ on item $i = 1,2 \ldots$

\[y_{mi} + \alpha_{m} + \mu_{i} + e_{mi}\]

\[e_{mi} \sim \mathcal{n} (0,\sigma^2_m)\]

Even though the separate variances can not be
identified, their sum can be estimated by the empirical variance of the differences.

Like wise the separate $\alpha$ can not be
estimated, only theiir difference can be estimated as
$\bar{D}$



\newpage
\chapter{LME models for MCS}
\section{Statement of the LME model}

Further to a paper published by Laird and Ware in $1982$, a linear mixed effects model is a linear mdoel that combined fixed and random effect terms formulated as follows;

  \begin{displaymath}
      Y_{i} =X_{i}\beta + Z_{i}b_{i} + \epsilon_{i}
  \end{displaymath}
\begin{itemize}

\item $Y_{i}$ is the $n \times 1$ response vector \item $X_{i}$ is
the $n \times p$ Model matrix for fixed effects \item $\beta$ is
the $p \times 1$ vector of fixed effects coefficients \item
$Z_{i}$ is the $n \times q$ Model matrix for random effects \item
$b_{i}$ is the $q \times 1$ vector of random effects coefficients,
sometimes denoted as $u_{i}$ \item $\epsilon$ is the $n \times 1$
vector of observation errors
\end{itemize}






%-----------------------------------------------------------------------------------------------------%
\newpage

\subsection{Bendix Carstensen's data sets}
\citet{bxc2008}describes the sampling method when discussing of a motivating example.Diabetes patients attending an outpatient clinic in Denmark have their $HbA_{1c}$ levels routinely measured at every visit.Venous and Capillary blood samples were obtained from all patients appearing at the clinic over two days.

Samples were measured on four consecutive days on each machines, hence there are five analysis days.Carstensen notes that every machine was calibrated every day to  the manufacturers guidelines.


\subsection{Limits of agreement for Carstensen's data}


\citet{bxc2008} describes the calculation of the limits of agreement (with the inter-method bias implicit) for both data sets, based on his formulation;

\[\hat{\alpha}_1 - \hat{\alpha}_2 \pm 2\sqrt{2\hat{\tau}^2 +\hat{\sigma}_1^2 +\hat{\sigma}_2^2 }.\]

For the `Fat' data set, the inter-method bias is shown to be $0.045$. The limits of agreement are $(-0.23 , 0.32)$

Carstensen demonstrates the use of the interaction term when computing the limits of agreement for the `Oximetry' data set. When the interaction term is omitted, the limits of agreement are $(-9.97, 14.81)$. Carstensen advises the inclusion of the interaction term for linked replicates, and hence the limits of agreement are recomputed as $(-12.18,17.12)$.


%-----------------------------------------------------------------------------------------------------%
\newpage

\subsection{Limits of Agreement in LME models}
\citet{bxc2008} uses LME models to determine the limits of agreement. Between-subject variation for method $m$ is given by $d^2_{m}$ and within-subject variation is given by $\lambda^2_{m}$.  \citet{BXC2008} remarks that for two methods $A$ and $B$, separate values of $d^2_{A}$ and $d^2_{B}$ cannot be estimated, only their average. Hence the assumption that $d_{x}= d_{y}= d$ is necessary. The between-subject variability $\boldsymbol{D}$ and within-subject variability $\boldsymbol{\Lambda}$ can be presented in matrix form,\[
\boldsymbol{D} = \left(%
\begin{array}{cc}
   d^2_{A}& 0 \\
  0 & d^2_{B} \\
\end{array}%
\right)=\left(%
\begin{array}{cc}
   d^2& 0 \\
  0 & d^2\\
\end{array}%
\right),
\hspace{1.5cm}
\boldsymbol{\Lambda} = \left(%
\begin{array}{cc}
   \lambda^2_{A}& 0 \\
  0 & \lambda^2_{B} \\
\end{array}%
\right).
\]

The variance for method $m$ is $d^2_{m}+\lambda^2_{m}$. Limits of agreement are determined using the standard deviation of the case-wise differences between the sets of measurements by two methods $A$ and $B$, given by
\begin{equation}
\mbox{var} (y_{A}-y_{B}) = 2d^2 + \lambda^2_{A}+ \lambda^2_{B}.
\end{equation}
Importantly the covariance terms in both variability matrices are zero, and no covariance component is present.


\citet{roy} has demonstrated a methodology whereby $d^2_{A}$ and $d^2_{B}$ can be estimated separately. Also covariance terms are present in both $\boldsymbol{D}$ and $\boldsymbol{\Lambda}$. Using Roy's methodology, the variance of the differences is
\begin{equation}
\mbox{var} (y_{iA}-y_{iB})= d^2_{A} + \lambda^2_{B} + d^2_{A} + \lambda^2_{B} - 2(d_{AB} + \lambda_{AB})
\end{equation}
All of these terms are given or determinable in computer output.
The limits of agreement can therefore be evaluated using
\begin{equation}
\bar{y_{A}}-\bar{y_{B}} \pm 1.96 \times \sqrt{ \sigma^2_{A} + \sigma^2_{B}  - 2(\sigma_{AB})}.
\end{equation}

For Carstensen's `fat' data, the limits of agreement computed using Roy's
method are consistent with the estimates given by \citet{BXC2008}; $0.044884  \pm 1.96 \times  0.1373979 = (-0.224,  0.314).$


%-----------------------------------------------------------------------------------------------------%
\newpage

\subsection{Repeatability}
Barnhart emphasizes the importance of repeatability as part of an overall method comparison study. Before there can be good agreement between two methods, a method must have good agreement with itself. The coefficient of repeatability , as proposed by \citet{BA99} is an important feature of both Carstensen's and Roy's methodologies. The coefficient is calculated from the residual standard deviation (i.e. $1.96 \times \sqrt{2} \times \sigma_m$ = $2.83 \sigma_m$).

%-----------------------------------------------------------------------------------------------------%
\newpage
\section{Hamlett and Lam}
The methodology proposed by \citet{Roy2009} is largely based on \citet{hamlett}, which in turn follows on from \citet{lam}.

%Lam 99
%In many cases, repeated observation are collected from each subject in sequence  and/or longitudinally.

%Hamlett
%Hamlett re-analyses the data of lam et al to generalize their model to cover other settings not covered by the Lam %method.


%-----------------------------------------------------------------------------------------------------%
\newpage
\subsection{Roy's variability tests}
Variability tests proposed by \citet{Roy2009} affords the opportunity to expand upon Carstensen's approach.

The first test allows of the comparison the begin-subject variability of two methods. Similarly, the second test
assesses the within-subject variability of two methods. A third test is a test that compares the overall variability of the two methods.

The tests are implemented by fitting a specific LME model, and three variations thereof, to the data. These three variant models introduce equality constraints that act null hypothesis cases.

Other important aspects of the method comparison study are consequent. The limits of agreement are computed using the results of the first model.
\subsection{Repeated Measurements }
In cases where there are repeated measurements by each of the two
methods on the same subjects , Bland Altman suggest calculating
the mean for each method on each subject and use these pairs of
means to compare the two methods.
\\
The estimate of bias will be unaffected using this approach, but
the estimate of the standard deviation of the differences will be
too small, because of the reduction of the effect of repeated
measurement error. Bland Altman propose a correction for this.
\\
Carstensen attends to this issue also, adding that another
approach would be to treat each repeated measurement separately.
\newpage
\section{Repeated Measurements}

In cases where there are repeated measurements by each of the two
methods on the same subjects , Bland Altman suggest calculating
the mean for each method on each subject and use these pairs of
means to compare the two methods.
The estimate of bias will be unaffected using this approach, but
the estimate of the standard deviation of the differences will be
too small, because of the reduction of the effect of repeated
measurement error. Bland Altman propose a correction for this.
Carstensen attends to this issue also, adding that another
approach would be to treat each repeated measurement separately.
\section{Carstensen's Mixed Models}

\citet{BXC2004} proposes linear mixed effects models for deriving
conversion calculations similar to Deming's regression, and for
estimating variance components for measurements by different
methods. The following model ( in the authors own notation) is
formulated as follows, where $y_{mir}$ is the $r$th replicate
measurement on subject $i$ with method $m$.

\begin{equation}
y_{mir}  = \alpha_{m} + \beta_{m}\mu_{i} + c_{mi} + e_{mir} \qquad
( e_{mi} \sim N(0,\sigma^{2}_{m}), c_{mi} \sim N(0,\tau^{2}_{m}))
\end{equation}
The intercept term $\alpha$ and the $\beta_{m}\mu_{i}$ term follow
from \citet{DunnSEME}, expressing constant and proportional bias
respectively , in the presence of a real value $\mu_{i}.$
 $c_{mi}$ is a interaction term to account for replicate, and
 $e_{mir}$ is the residual associated with each observation.
Since variances are specific to each method, this model can be
fitted separately for each method.

The above formulation doesn't require the data set to be balanced.
However, it does require a sufficient large number of replicates
and measurements to overcome the problem of identifiability. The
import of which is that more than two methods of measurement may
be required to carry out the analysis. There is also the
assumptions that mobservations of measurements by particular
methods are exchangeable within subjects. (Exchangeability means
that future samples from a population behaves like earlier
samples).

%\citet{BXC2004} describes the above model as a `functional model',
%similar to models described by \citet{Kimura}, but without any
%assumptions on variance ratios. A functional model is . An
%alternative to functional models is structural modelling

\citet{BXC2004} uses the above formula to predict observations for
a specific individual $i$ by method $m$;

\begin{equation}BLUP_{mir} = \hat{\alpha_{m}} + \hat{\beta_{m}}\mu_{i} +
c_{mi} \end{equation}. Under the assumption that the $\mu$s are
the true item values, this would be sufficient to estimate
parameters. When that assumption doesn't hold, regression techniques (known as updating techniques)
can be used additionally to determine the estimates.
The assumption of exchangeability can be unrealistic in certain situations.
\citet{BXC2004} provides an amended formulation which includes an extra interaction
term ($d_{mr} d_{mr} \sim N(0,\omega^{2}_{m}$)to account for this.

\citet{BXC2008} sets out a methodology of computing the limits of
agreement based upon variance component estimates derived using
linear mixed effects models. Measures of repeatability, a
characteristic of individual methods of measurements, are also
derived using this method.

\subsection{Using LME models to create Prediction Intervals}
\citet{BXC2004} also advocates the use of linear mixed models in
the study of method comparisons. The model is constructed to
describe the relationship between a value of measurement and its
real value. The non-replicate case is considered first, as it is
the context of the Bland Altman plots. This model assumes that
inter-method bias is the only difference between the two methods.
A measurement $y_{mi}$ by method $m$ on individual $i$ is
formulated as follows;
\begin{equation}
y_{mi}  = \alpha_{m} + \mu_{i} + e_{mi} \qquad ( e_{mi} \sim
N(0,\sigma^{2}_{m}))
\end{equation}
The differences are expressed as $d_{i} = y_{1i} - y_{2i}$ For the
replicate case, an interaction term $c$ is added to the model,
with an associated variance component. All the random effects are
assumed independent, and that all replicate measurements are
assumed to be exchangeable within each method.

\begin{equation}
y_{mir}  = \alpha_{m} + \mu_{i} + c_{mi} + e_{mir} \qquad ( e_{mi}
\sim N(0,\sigma^{2}_{m}), c_{mi} \sim N(0,\tau^{2}_{m}))
\end{equation}

\citet{BXC2008} proposes a methodology to calculate prediction
intervals in the presence of replicate measurements, overcoming
problems associated with Bland-Altman methodology in this regard.
It is not possible to estimate the interaction variance components
$\tau^{2}_{1}$ and $\tau^{2}_{2}$ separately. Therefore it must be
assumed that they are equal. The variance of the difference can be
estimated as follows:
\begin{equation}
var(y_{1j}-y_{2j})
\end{equation}

\subsection{Computation} Modern software
packages can be used to fit models accordingly. The best linear
unbiased predictor (BLUP) for a specific subject $i$ measured with
method $m$ has the form $BLUP_{mir} = \hat{\alpha_{m}} +
\hat{\beta_{m}}\mu_{i} + c_{mi}$, under the assumption that the
$\mu$s are the true item values.


\subsection{Repeated Measurements }
In cases where there are repeated measurements by each of the two
methods on the same subjects , Bland Altman suggest calculating
the mean for each method on each subject and use these pairs of
means to compare the two methods.
\\
The estimate of bias will be unaffected using this approach, but
the estimate of the standard deviation of the differences will be
too small, because of the reduction of the effect of repeated
measurement error. Bland Altman propose a correction for this.
\\
Carstensen attends to this issue also, adding that another
approach would be to treat each repeated measurement separately.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this model , the variances of the random effects must depend on
$m$, since the different methods do not necessarily measure on the
same scale, and different methods naturally must be assumed to
have different variances. \citet{BXC2004} attends to the issue of
comparative variances.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Other Approaches


\newpage



\citet{pkcng} generalize this approach to account for situations
where the distributions are not identical, which is commonly the
case. The TDI is not consistent and may not preserve its
asymptotic nominal level, and that the coverage probability
approach of \citet{lin2002} is overly conservative for moderate
sample sizes. This methodology proposed by \citet{pkcng} is a
regression based approach that models the mean and the variance of
differences as functions of observed values of the average of the
paired measurements.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

Maximum likelihood estimation is used to estimate the parameters.
The REML estimation is not considered since it does not lead to a
joint distribution of the estimates of fixed effects and random
effects parameters, upon which the assessment of agreement is
based.

\section{Random Effects and MCS}
The methodology comprises two calculations. The second calculation
is for the standard deviation of means Before the modified Bland
and Altman method can be applied for repeated measurement data, a
check of the assumption that the variance of the repeated
measurements for each subject by each method is independent of the
mean of the repeated measures. This can be done by plotting the
within-subject standard deviation against the mean of each subject
by each method. Mean Square deviation measures the total deviation
of a


\subsection{Random coefficient growth curve model} (Chincilli
1996) Random coefficient growth curve model, a special type of
mixed model have been proposed a single measure of agreement for
repeated measurements.
\begin{equation}
\textbf{d}= \textbf{Xb} + \textbf{Zu} + \textbf{e}
\end{equation}
The distributional asummptions also require \textbf{d} to
\textbf{N}

%------------------------------------------------------------------
\newpage
\section{Other Approaches}

\subsection{Random coefficient growth curve model} (Chincilli
1996) Random coefficient growth curve model, a special type of
mixed model have been proposed  a single measure of agreement for
repeated measurements.
\subsection{Marginal Modelling}
(Diggle 2002) proposes the use of marginal models as an
alternative to mixed models.m Marginal models are appropriate when
interences about the mean response are of specific interest.
\section{KP}
Most residual covariance structures are design for one
within-subject factor. However two or more may be present. For
such cases,an approppriate approach would be the residual
covariance structure using Kronecker product of the underlying
within-subject factor specific covariances structure.

\section{LME}
Consistent with the conventions of mixed models, \citet{pkc}
formulates the measurement $y_{ij} $from method $i$ on individual
$j$ as follows;
\begin{equation}
y_{ij} =P_{ij}\theta + W_{ij}v_{i} + X_{ij}b_{j} + Z_{ij}u_{j} +
\epsilon_{ij},     (j=1,2, i=1,2....n)
\end{equation}
The design matrix $P_{ij}$ , with its associated column vector
$\theta$, specifies the fixed effects common to both methods. The
fixed effect specific to the $j$th method is articulated by the
design matrix $W_{ij}$ and its column vector $v_{i}$. The random
effects common to both methods is specified in the design matrix
$X_{ij}$, with vector $b_{j}$ whereas the random effects specific
to the $i$th subject by the $j$th method is expressed by $Z_{ij}$,
and vector $u_{j}$. Noticeably this notation is not consistent
with that described previously.  The design matrices are specified
so as to includes a fixed intercept for each method, and a random
intercept for each individual. Additional assumptions must also be
specified;
\begin{equation}
v_{ij} \sim N(0,\Sigma),
\end{equation}
These vectors are assumed to be independent for different $i$s,
and are also mutually independent. All Covariance matrices are
positive definite.  In the above model effects can be classed as
those common to both methods, and those that vary with method.
When considering differences, the effects common to both
effectively cancel each other out. The differences of each pair of
measurements can be specified as following;
\begin{equation}
d_{ij} = X_{ij}b_{j} + Z_{ij}u_{j} + \epsilon_{ij},     (j=1,2,
i=1,2....n)
\end{equation}
This formulation has seperate distributional assumption from the
model stated previously.

This agreement covariate $x$ is the key step in how this
methodology assesses agreement.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5



%-----------------------------------------------------------------------------------------------------%
\newpage
\bibliography{DB-txfrbib}
\end{document}