% http://www.math.ku.dk/~erhansen/web/stat1/pinheiro.pdf  -IMPORTANT
\documentclass[12pt, a4paper]{article}
\usepackage{natbib}
\usepackage{vmargin}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{amsbsy}
\usepackage{amsthm, amsmath}
%\usepackage[dvips]{graphicx}
\bibliographystyle{chicago}
\renewcommand{\baselinestretch}{1.4}

% left top textwidth textheight headheight % headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{23.5cm}{0.5cm}{0cm}{1cm}{1cm}

\pagenumbering{arabic}


\begin{document}
\author{Kevin O'Brien}
\title{Introduction to LME Models}

\begin{enumerate}
\item Classical models
\item Grouped data sets
\item variance components
\begin{enumerate}
\item Fixed effects
\item Random effects
\end{enumerate}
\end{enumerate}

\section{Classical Models}
%http://www.spss.ch/upload/1126184451_Linear%20Mixed%20Effects%20Modeling%20in%20SPSS.pdf

A fitted model has the form $y\;=\;X\beta+\varepsilon$, where $y$ is a vector of responses, $X$ is the fixed-effects design matrix, $\beta$ is a 
vector of fixed-effects parameters and $\boldsymbol{\varepsilon}}$ is a vector of residual errors.

%LAter

\subsection{With non-iid residual errors}
%spss.ch
%Growth Data Set?
The assumption may be violated in some situations. This often happens when repeated measurements are made on each 
subject. In the \textit{growth} study dataset, for example, the response variable of each subject is measured at various ages. We 
may suspect that error terms within a subject are correlated. A reasonable choice of the residual error covariance will therefore 
be a block diagonal matrix, where each block is a first-order autoregressive (AR1) covariance matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Grouped Data Sets}
In modern statistical analysis , data sets have very complex
structures, such as  clustered data, repeated data and
hierarchical data (henceforth referred to as grouped data).

Repeated data considers various observations periodically taken
from the same subjects. `Before and after' measurements, as used
in paired t tests, are a well known example of repeated
measurements. Clustered data is simply the grouping of
observations according to common characteristics. For example, an
study of pupils of a school would account for the fact that they
are grouped according to their classes.

Hierarchical structures organize data into a tree-like structure,
i.e. groups within groups. Using the previous example, the pupils
would be categorized according to their years (i.e the parent
group) and then their classes (i.e the child group). This can be
extended again to multiple schools, where each school would be the
parent group of each year.

An important feature of such data sets is that there is
correlation between observations within each of the groups
\citep{Faraway}. Observations in different groups may be
independent, but any assumption that these observations within the
same group are independent is inappropriate . Consequently
\citet{Demi} states that there is two sources of variations to be
considered, `within groups' and `between groups'.


\section{Normal linear mixed models}

The standard linear mixed effects model specifies
\begin{equation}
y = X \beta + Zb + \epsilon , 
\label{lme:Model}
\end{equation}
where $y$ is a vector of $N$ observable random variables, $\beta$ is a vector of $p$ unknown parameters having fixed values (fixed effects), $X$ and $Z$ are $N \times p$ and $N \times q$ known matrices, and $b$ and $\epsilon$  are vectors of $q$ and $N,$ respectively, unobservable random variables (random effects) such that $\mathrm{E}(b)=0, \ \mathrm{E}(\epsilon)=0$
and
\[
\mathrm{var}
\pmatrix{
  b \cr
  \epsilon }  =
\pmatrix{
  D & 0 \cr
  0 & \Sigma }
\]
where $D$ and $\Sigma$ are positive definite matrices parameterized by an unknown variance component parameter vector $ \theta.$


\subsubsection*{Henderson's equations}

\cite{Henderson:1950} made the (ad-hoc) distributional assumptions $y|b \sim \mathrm{N} (X \beta + Zb, \Sigma)$ and $b \sim \mathrm{N}(0,D),$ and proceeded to maximize the joint density of $y$ and $b$
\begin{equation}
\left|
\pmatrix{
  D & 0 \cr
  0 & \Sigma }
  \right|^{-\frac{1}{2}}
\exp
\left\{ -\frac{1}{2}
\pmatrix{
  b \cr
  y - X \beta -Zb
  }^\prime
\pmatrix{
  D & 0 \cr
  0 & \Sigma }^{-1}
\pmatrix{
  b \cr
  y - X \beta -Zb
  }
\right\},
\label{u&beta:JointDensity}
\end{equation}
with respect to $\beta$ and $b,$ which ultimately requires minimizing the criterion
\begin{equation}
(y - X \beta -Zb)'\Sigma^{-1}(y - X \beta -Zb) + b^\prime D^{-1}b. 
\label{Henderson:Criterion}
\end{equation}
This leads to the solutions
\begin{equation}
\pmatrix{
  X^\prime\Sigma^{-1}X & X^\prime\Sigma^{-1}Z
  \cr
  Z^\prime\Sigma^{-1}X & X^\prime\Sigma^{-1}X + D^{-1}
  }
\pmatrix{
    \beta \cr
  b
  }
  =
\pmatrix{
  X^\prime\Sigma^{-1}y \cr
  Z^\prime\Sigma^{-1}y
  }.
\label{Henderson:Equations}
\end{equation}
\cite{Robi:BLUP:1991} points out that although \cite{Henderson:1950} initially referred to the estimates $\hat{\beta}$ and $\hat{b}$ from (\ref{Henderson:Equations}) as ``joint maximum likelihood estimates" \cite{Henderson:1973} later advised that these estimates should not be referred to as ``maximum likelihood" as the function being maximized in (\ref{Henderson:Criterion}) is a joint density rather than a likelihood function.

\subsubsection*{Estimation of the fixed parameters}

The vector $y$ has marginal density $y \sim \mathrm{N}(X \beta,V),$ where $V = \Sigma + ZDZ^\prime$ is specified through the variance component parameters $\theta.$ The log-likelihood of the fixed parameters $(\beta, \theta)$ is
\begin{equation}
\ell (\beta, \theta|y) =
-\frac{1}{2} \log |V| -\frac{1}{2}(y -
X \beta)'V^{-1}(y -
X \beta), \label{Likelihood:MarginalModel}
\end{equation}
and for fixed $\theta$ the estimate $\hat{\beta}$ of $\beta$ is obtained as the solution of
\begin{equation}
(X^\prime V^{-1}X) {\beta} = X^\prime V^{-1}y.
\label{mle:beta:hat}
\end{equation} 

Maximum likelihood and restricted maximum likelihood have become the most common strategies for estimating the variance component parameter $\theta.$ Substituting $\hat{\beta}$ from (\ref{mle:beta:hat}) into $\ell(\beta, \theta|y)$ from (\ref{Likelihood:MarginalModel}) returns the \emph{profile} log-likelihood
\begin{eqnarray*}
\ell_P(\theta \mid y) &=& \ell(\hat{\beta}, \theta \mid y) \\ 
&=& -\frac{1}{2} \log |V| -\frac{1}{2}(y - X \hat{\beta})'V^{-1}(y - X \hat{\beta})
\end{eqnarray*}
of the variance parameter $\theta.$ Estimates of the parameters $\theta$ specifying $V$ can be found by maximizing $\ell_P(\theta \mid y)$ over $\theta.$ In practice the \emph{restricted} log-likelihood
\[
\ell_R(\theta \mid y) =
\ell_P(\theta \mid y) -\frac{1}{2} \log |X^\prime VX |
\]
is preferred. This approach is based on maximizing the likelihood of linear combinations of $y$ that do not depend on $\beta,$ and in this way takes into account the estimation of $\beta.$


\subsubsection*{Estimation of the random effects}

The established approach for estimating the random effects is to use the best linear predictor of $b$ from $y,$ which for a given $\beta$ equals $DZ^\prime V^{-1}(y - X \beta).$ In practice $\beta$ is replaced by an estimator such as $\hat{\beta}$ from (\ref{mle:beta:hat}) so that $\hat{b} = DZ^\prime V^{-1}(y - X \hat{\beta}).$ Pre-multiplying by the appropriate matrices it is straightforward to show that these estimates $\hat{\beta}$ and $\hat{b}$ satisfy the equations in (\ref{Henderson:Equations}).


\subsubsection*{The extended likelihood}

The desire to have an entirely likelihood-based justification for estimates of random effects has motivated \citet[page 429]{Pawi:in:2001} to define the \emph{extended likelihood}. He remarks ``In mixed effects modelling the extended likelihood has been called \emph{h-likelihood} (for hierarchical  likelihood) by \cite{Lee:Neld:hier:1996}, while in smoothing literature it is known as the \emph{penalized likelihood} (e.g.\ \citeauthor{Gree:Silv:nonp:1994} \citeyear{Gree:Silv:nonp:1994})." The extended likelihood can be written $L(\beta,\theta,b|y) = p(y|b;\beta,\theta) p(b;\theta)$ and adopting the same distributional assumptions used by \cite{Henderson:1950} yields the log-likelihood function
\begin{eqnarray*}
\ell_h(\beta,\theta,b|y)
& = \displaystyle -\frac{1}{2} \left\{ \log|\Sigma| + (y - X \beta -Zb)'\Sigma^{-1}( y - X \beta -Zb) \right.\\
&  \hspace{0.5in} \left. + \log|D| + b^\prime D^{-1}b \right\}.
\end{eqnarray*}
Given $\theta$, differentiating with respect to $\beta$ and $b$ returns Henderson's equations in (\ref{Henderson:Equations}).


Henderson's equations in (\ref{Henderson:Equations}) can be rewritten $( T^\prime W^{-1} T ) \delta = T^\prime W^{-1} y_{a} $ using
\[
\delta = \pmatrix{\beta \cr b},
\ y_{a} = \pmatrix{
  y \cr \psi
  },
\ T = \pmatrix{
  X & Z  \cr
  0 & I
  },
\ \textrm{and} \ W = \pmatrix{
  \Sigma & 0  \cr
  0 &  D },
\]
where \cite{Lee:Neld:Pawi:2006} describe $\psi = 0$ as quasi-data with mean $\mathrm{E}(\psi) = b.$ Their formulation suggests that the joint estimation of the coefficients $\beta$ and $b$ of the linear mixed effects model in (\ref{lme:Model}) can be derived via a classical augmented general linear model $y_{a} = T\delta + \varepsilon$ where $\mathrm{E}(\varepsilon) = 0$ and $\mathrm{var}(\varepsilon) = W,$ with \emph{both} $\beta$ and $b$ appearing as fixed parameters.

\section{Note 2: Model terms}
It is important to note the following characteristics of this model.
\begin{itemize}
\item Let the number of replicate measurements on each item $i$ for both methods be $n_i$, hence $2 \times n_i$ responses. However, it is assumed that there may be a different number of replicates made for different items. Let the maximum number of replicates be $p$. An item will have up to $2p$ measurements, i.e. $\max(n_{i}) = 2p$.

% \item $\boldsymbol{y}_i$ is the $2n_i \times 1$ response vector for measurements on the $i-$th item.
% \item $\boldsymbol{X}_i$ is the $2n_i \times  3$ model matrix for the fixed effects for observations on item $i$.
% \item $\boldsymbol{\beta}$ is the $3 \times  1$ vector of fixed-effect coefficients, one for the true value for item $i$, and one effect each for both methods.

\item Later on $\boldsymbol{X}_i$ will be reduced to a $2 \times 1$ matrix, to allow estimation of terms. This is due to a shortage of rank. The fixed effects vector can be modified accordingly.
\item $\boldsymbol{Z}_i$ is the $2n_i \times  2$ model matrix for the random effects for measurement methods on item $i$.
\item $\boldsymbol{b}_i$ is the $2 \times  1$ vector of random-effect coefficients on item $i$, one for each method.
\item $\boldsymbol{\epsilon}$  is the $2n_i \times  1$ vector of residuals for measurements on item $i$.
\item $\boldsymbol{G}$ is the $2 \times  2$ covariance matrix for the random effects.
\item $\boldsymbol{R}_i$ is the $2n_i \times  2n_i$ covariance matrix for the residuals on item $i$.
\item The expected value is given as $\mbox{E}(\boldsymbol{y}_i) = \boldsymbol{X}_i\boldsymbol{\beta}.$ \citep{hamlett}
\item The variance of the response vector is given by $\mbox{Var}(\boldsymbol{y}_i)  = \boldsymbol{Z}_i \boldsymbol{G} \boldsymbol{Z}_i^{\prime} + \boldsymbol{R}_i$ \citep{hamlett}.
\end{document} 
