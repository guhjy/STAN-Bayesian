
\documentclass[12pt, a4paper]{report}
\usepackage{natbib}
\usepackage{vmargin}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{amsbsy}
\usepackage{amsthm, amsmath}
%\usepackage[dvips]{graphicx}
\bibliographystyle{chicago}
\renewcommand{\baselinestretch}{1.1}

% left top textwidth textheight headheight % headsep footheight footskip
\setmargins{2.0cm}{2cm}{15.5 cm}{23.5cm}{0.5cm}{0cm}{1cm}{1cm}

\pagenumbering{arabic}


\begin{document}
\author{Kevin O'Brien}
\title{Additions}
\date{\today}
\maketitle

\tableofcontents \setcounter{tocdepth}{2}

\begin{itemize}
\item LMEs
\item Likelihood and and log likelihood functions
\item Likelihood ratio test
\item more on score functions etc
\item MLEs
\item Algorithms

\end{itemize}

The estimate for the fixed effects are referred to as the best linear unbiased estimates (BLUE). Henderson's estimate for the random effects is known as the best linear unbiased predictor (BLUP).

\subsection{Likelihood and estimation}

Likelihood is the hypothetical probability that an event that has
already occurred would yield a specific outcome. Likelihood
differs from probability in that probability refers to future
occurrences, while likelihood refers to past known outcomes.

The likelihood function is a fundamental concept in statistical
inference. It indicates how likely a particular population is to
produce an observed sample. The set of values that maximize the
likelihood function are considered to be optimal, and are used as
the estimates of the parameters.

Maximum likelihood (ML) estimation is a method of obtaining
parameter estimates by optimizing the likelihood function. The
likelihood function is constructed as a function of the parameters
in the specified model.

Restricted maximum likelihood (REML) is an alternative methods of
computing parameter estimated. REML is often preferred to ML
because it produces unbiased estimates of covariance parameters by
taking into account the loss of degrees of freedom that results
from estimating the fixed effects in $\boldsymbol{\beta}$.

REML estimation reduces the bias in the variance component, and also handles high correlations
more effectively, and is less sensitive to outliers than ML.  The problem with REML for model building is that the "likelihoods" obtained for different fixed effects are not comparable. Hence it is not valid to compare models
with different fixed effects using a likelihood ratio test or AIC when REML is used to
estimate the model. Therefore models derived using ML must be used instead.
\newpage



\newpage
Assuming a statistical model $f_{\theta}(y)$ parameterized by a fixed and unknown set of parameters $\theta$, the likelihood $L(\theta)$ is the probability of the observed data $y$ considered as a function of $\theta$ \citep{youngjo}.

The log likelihood $\emph{l}(\theta)$

\newpage
\section{Likelihood ratio tests}
Likelihood ratio tests are  a class o tests based on the
comparison of the values of the likelihood functions of two
candidate models. LRTs can be used to test hypotheses about
covariance parameters or fixed effects parameters in the context
of LMEs.

The test statistic for the LRT is the difference of the log-likelihood functions, multiplied by $-2$.
The probability distribution of the test statistic is approximated by the $\chi^2$ distribution with ($\nu_{1} - \nu_{2}$) degrees of freedom, where $\nu_{1}$  and $\nu_{2}$ are the degrees of freedom of models 1 and 2 respectively.

The score function $S(\theta)$ is the derivative of the log likelihood with respect to $\theta$,

\[
S(\theta) = \frac{\partial}{\partial \theta}\emph{l}(\theta),
\]

and the maximum likelihood estimate is the solution to the score equation
\[
S(\theta) = 0.
\]
The Fisher information $I(\theta)$, which is defined as
\[
I(\theta) = - \frac{\partial^2}{\partial \theta^2}\emph{l}(\theta),
\]
give rise to the observed Fisher information ($I(\hat{\theta})$) and the expected Fisher information ($\mathcal{I}(\theta)$).


\newpage
\citet{Lam} used ML estimation to estimate the true correlation between the variables when
the measurements are linked over time. The methodology relies on the assumption that the two variables with repeated measures follow a multivariate normal distribution. The methodology currently does not extend to any more than two cases. The MLE of the correlation takes into account the dependency among repeated measures.

The true correlation $\rho_{xy}$ is repeated measurements can be considered as having two components: between subject and within-subject correlation. The usefulness of estimating repeated measure correlation coefficients is the calculation of between-method and within-method variabilities are produced as by-products.

\end{document}

\newpage



\subsection{Test for inter-method bias}
Bias is determinable by examination of the 't-table'. Estimate for both methods are given, and the bias is simply the difference between the two. Because the $R$ implementation does not account for an intercept term, a $p-$value is not given. Should a $p-$value be required specifically for the bias, and simple restructuring of the model is required wherein an intercept term is included. Output from a second implementation will yield a $p-$value.
\newpage


